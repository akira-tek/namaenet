{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FKdSSZ5-2osa",
        "outputId": "41112eb3-c02c-4c7f-c7f0-319248223f92"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.6.0+cu124'"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_Ie2fbQ2vcu"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpHQ3oaR2sCj",
        "outputId": "bae87aff-2efe-4025-f0ea-7ea57b5d8724"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([182516, 8]), torch.Size([45630, 8]))"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ------------------- preparation -------------------\n",
        "names = open(\"names.txt\", \"r\", encoding=\"utf-8\").read().splitlines()\n",
        "chars = sorted(set(''.join(names)))\n",
        "ctoi = { ch:i+1 for i, ch in enumerate(chars) }\n",
        "ctoi['.'] = 0\n",
        "itoc = { i:ch for ch, i in ctoi.items() }\n",
        "\n",
        "# ------------------- modules -------------------\n",
        "encode = lambda string: torch.tensor([ctoi[char] for char in string])\n",
        "decode = lambda tensor: ''.join(itoc[idx.item()] for idx in tensor)\n",
        "\n",
        "def build_dataset(data: list, context_size: int):\n",
        "  features, labels = [], []\n",
        "\n",
        "  for word in data:\n",
        "    padded_word = word + '.'\n",
        "    indices = [ctoi[char] for char in padded_word]\n",
        "    context = [0] * context_size + indices\n",
        "\n",
        "    for i in range(len(indices)):\n",
        "      sequence = context[i : i + context_size + 1]\n",
        "      features.append(sequence[:-1])\n",
        "      labels.append(sequence[1:])\n",
        "\n",
        "  return torch.tensor(features, dtype=torch.long), torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "def get_batch(features: torch.Tensor, labels: torch.Tensor, batch_size: int):\n",
        "  batch_indices = torch.randint(0, features.shape[0], size=(batch_size,))\n",
        "\n",
        "  return features[batch_indices], labels[batch_indices]\n",
        "\n",
        "# ------------------- datasets -------------------\n",
        "features, labels = build_dataset(names, context_size=8)\n",
        "\n",
        "num_train = int(0.8 * features.shape[0])\n",
        "train_features, train_labels = features[:num_train], labels[:num_train]\n",
        "val_features, val_labels = features[num_train:], labels[num_train:]\n",
        "\n",
        "train_features.shape, val_features.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MOjVshi6njn"
      },
      "source": [
        "# Custom Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDwT9xGC6zc4"
      },
      "source": [
        "Modules required for the Transformer architecture:\n",
        "- Head (the basic unit)\n",
        "- MultiHeadAttention (multiple heads + projection + dropout)\n",
        "- FeedForward (a simple neural net)\n",
        "- Block (MultiHeadAttention + FeedForward using skip connections and layernorms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "g2FtbgaC7Hx1"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    def __init__(self, embedding_dim: int, head_size: int, dropout_p: float = 0.2):\n",
        "        super().__init__()\n",
        "        self.head_size = head_size\n",
        "        self.query = nn.Linear(embedding_dim, head_size, bias=False)\n",
        "        self.key = nn.Linear(embedding_dim, head_size, bias=False)\n",
        "        self.value = nn.Linear(embedding_dim, head_size, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        B, T, C = x.shape\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "\n",
        "        scores = q @ k.transpose(-2, -1) * (self.head_size**-0.5)\n",
        "\n",
        "        tril = torch.tril(torch.ones(T, T))\n",
        "        scores = scores.masked_fill(tril == 0, float(\"-inf\"))\n",
        "\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        out = attn @ v\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_heads: int,\n",
        "        head_size: int,\n",
        "        embedding_dim: int,\n",
        "        dropout_p: float = 0.2\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(embedding_dim, head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(num_heads * head_size, embedding_dim)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "        x = self.dropout(self.proj(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embedding_dim: int, dropout_p: float = 0.2):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, embedding_dim*4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embedding_dim*4, embedding_dim),\n",
        "            nn.Dropout(dropout_p)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dim: int,\n",
        "        num_heads: int,\n",
        "        head_size: int\n",
        "    ):\n",
        "      super().__init__()\n",
        "      self.self_attention = MultiHeadAttention(\n",
        "          num_heads=num_heads,\n",
        "          head_size=head_size,\n",
        "          embedding_dim=embedding_dim\n",
        "          )\n",
        "      self.feed_forward = FeedForward(embedding_dim)\n",
        "      self.layernorm1 = nn.LayerNorm(embedding_dim)\n",
        "      self.layernorm2 = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "      x = x + self.self_attention(self.layernorm1(x))\n",
        "      x = x + self.feed_forward(self.layernorm2(x))\n",
        "      return x\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        context_size: int,\n",
        "        embedding_dim: int,\n",
        "        num_blocks: int,\n",
        "        num_heads: int,\n",
        "        head_size: int\n",
        "    ):\n",
        "      super().__init__()\n",
        "      self.context_size = context_size\n",
        "\n",
        "      self.token_embedding_table = nn.Embedding(vocab_size, embedding_dim)\n",
        "      self.position_embedding_table = nn.Embedding(context_size, embedding_dim)\n",
        "\n",
        "      self.blocknet = nn.Sequential(*[Block(embedding_dim, num_heads, head_size) for _ in range(num_blocks)])\n",
        "\n",
        "      self.layernorm = nn.LayerNorm(embedding_dim)\n",
        "      self.decoder = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, index: torch.Tensor, targets: torch.Tensor = None):\n",
        "      B, T = index.shape\n",
        "\n",
        "      token_emb = self.token_embedding_table(index)\n",
        "      position_emb = self.position_embedding_table(torch.arange(T))\n",
        "      x = token_emb + position_emb\n",
        "\n",
        "      x = self.layernorm(self.blocknet(x))\n",
        "      logits = self.decoder(x)\n",
        "\n",
        "      if targets is None:\n",
        "        loss = None\n",
        "      else:\n",
        "        B, T, C = logits.shape\n",
        "        logits = logits.view(B*T, C)\n",
        "        targets = targets.view(B*T)\n",
        "        loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "      return logits, loss\n",
        "\n",
        "    def generate(self, num_tokens: int):\n",
        "      tokens = torch.zeros((1, 1), dtype=torch.long)\n",
        "\n",
        "      self.eval()\n",
        "      with torch.inference_mode():\n",
        "        for _ in range(num_tokens):\n",
        "          output = []\n",
        "          context = [0] * self.context_size\n",
        "          while True:\n",
        "            logits, _ = self(torch.tensor([context]))\n",
        "            probabilities = F.softmax(logits[:, -1, :], dim=-1)\n",
        "            index = torch.multinomial(probabilities, num_samples=1)\n",
        "\n",
        "            output.append(index.item())\n",
        "            context = context[1:] + [index]\n",
        "\n",
        "            if index == 0:\n",
        "              break\n",
        "\n",
        "        print(''.join(itoc[index] for index in output))\n",
        "\n",
        "    def fit(self,\n",
        "            features: torch.Tensor,\n",
        "            labels: torch.Tensor,\n",
        "            optimizer: torch.optim.Optimizer,\n",
        "            batch_size: int,\n",
        "            epochs: int = 1,\n",
        "            verbose_frequency: int | None = None):\n",
        "      if verbose_frequency is None:\n",
        "        verbose_frequency = epochs // 10\n",
        "\n",
        "      for epoch in range(epochs):\n",
        "        features_batch, labels_batch = get_batch(features, labels, batch_size)\n",
        "        _, loss = self(features_batch, labels_batch)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % verbose_frequency == 0 or epoch+1==epochs:\n",
        "          print(f\"Epoch {epoch:6d}/{epochs} - loss: {loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exEp2FF987JM"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAIb7InQ881c",
        "outputId": "dff98fd5-383d-4648-8d57-9b3ff15742c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Net(\n",
              "  (token_embedding_table): Embedding(27, 128)\n",
              "  (position_embedding_table): Embedding(8, 128)\n",
              "  (blocknet): Sequential(\n",
              "    (0): Block(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x Head(\n",
              "            (query): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (key): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (layernorm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (layernorm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (1): Block(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x Head(\n",
              "            (query): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (key): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (layernorm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (layernorm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (2): Block(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x Head(\n",
              "            (query): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (key): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (layernorm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (layernorm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (3): Block(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x Head(\n",
              "            (query): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (key): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (layernorm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (layernorm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "  (decoder): Linear(in_features=128, out_features=27, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_size = len(chars) + 1 # plus the beginning/ending symbol\n",
        "context_size = 8\n",
        "embedding_dim = 128\n",
        "num_blocks = 4\n",
        "num_heads = 4\n",
        "head_size = embedding_dim // num_heads\n",
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 5000\n",
        "\n",
        "model = Net(vocab_size, context_size, embedding_dim, num_blocks, num_heads, head_size)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euN5LBFBLb6t",
        "outputId": "01b1a5f2-2d2a-4eb8-c71d-a86ba36d4435"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch      0/5000 - loss: 3.5886\n",
            "Epoch    100/5000 - loss: 1.5931\n",
            "Epoch    200/5000 - loss: 1.5996\n",
            "Epoch    300/5000 - loss: 1.3883\n",
            "Epoch    400/5000 - loss: 1.4831\n",
            "Epoch    500/5000 - loss: 1.3481\n",
            "Epoch    600/5000 - loss: 1.4135\n",
            "Epoch    700/5000 - loss: 1.5672\n",
            "Epoch    800/5000 - loss: 1.4822\n",
            "Epoch    900/5000 - loss: 1.4873\n",
            "Epoch   1000/5000 - loss: 1.4314\n",
            "Epoch   1100/5000 - loss: 1.3500\n",
            "Epoch   1200/5000 - loss: 1.4007\n",
            "Epoch   1300/5000 - loss: 1.4380\n",
            "Epoch   1400/5000 - loss: 1.4484\n",
            "Epoch   1500/5000 - loss: 1.2852\n",
            "Epoch   1600/5000 - loss: 1.4804\n",
            "Epoch   1700/5000 - loss: 1.2521\n",
            "Epoch   1800/5000 - loss: 1.5195\n",
            "Epoch   1900/5000 - loss: 1.3919\n",
            "Epoch   2000/5000 - loss: 1.3911\n",
            "Epoch   2100/5000 - loss: 1.4152\n",
            "Epoch   2200/5000 - loss: 1.3729\n",
            "Epoch   2300/5000 - loss: 1.4429\n",
            "Epoch   2400/5000 - loss: 1.4225\n",
            "Epoch   2500/5000 - loss: 1.3627\n",
            "Epoch   2600/5000 - loss: 1.4225\n",
            "Epoch   2700/5000 - loss: 1.4119\n",
            "Epoch   2800/5000 - loss: 1.4000\n",
            "Epoch   2900/5000 - loss: 1.4459\n",
            "Epoch   3000/5000 - loss: 1.5793\n",
            "Epoch   3100/5000 - loss: 1.4346\n",
            "Epoch   3200/5000 - loss: 1.3170\n",
            "Epoch   3300/5000 - loss: 1.4754\n",
            "Epoch   3400/5000 - loss: 1.4623\n",
            "Epoch   3500/5000 - loss: 1.4859\n",
            "Epoch   3600/5000 - loss: 1.4023\n",
            "Epoch   3700/5000 - loss: 1.2492\n",
            "Epoch   3800/5000 - loss: 1.3210\n",
            "Epoch   3900/5000 - loss: 1.3727\n",
            "Epoch   4000/5000 - loss: 1.4352\n",
            "Epoch   4100/5000 - loss: 1.4346\n",
            "Epoch   4200/5000 - loss: 1.3784\n",
            "Epoch   4300/5000 - loss: 1.2921\n",
            "Epoch   4400/5000 - loss: 1.2975\n",
            "Epoch   4500/5000 - loss: 1.2739\n",
            "Epoch   4600/5000 - loss: 1.3139\n",
            "Epoch   4700/5000 - loss: 1.4989\n",
            "Epoch   4800/5000 - loss: 1.3455\n",
            "Epoch   4900/5000 - loss: 1.4574\n",
            "Epoch   4999/5000 - loss: 1.4287\n"
          ]
        }
      ],
      "source": [
        "model.fit(train_features,\n",
        "          train_labels,\n",
        "          optimizer,\n",
        "          batch_size,\n",
        "          epochs,\n",
        "          verbose_frequency = 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5onqDW4Muwx",
        "outputId": "34bddc70-3d59-4eea-fdb8-715db695b5db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "effran.\n",
            "wenid.\n",
            "allian.\n",
            "yahri.\n",
            "aramir.\n",
            "zabaariyah.\n",
            "graisetu.\n",
            "omiyah.\n",
            "sh.\n",
            "brazell.\n",
            "kiplrin.\n",
            "brie.\n",
            "yasia.\n",
            "shanvik.\n",
            "prystine.\n",
            "ahmia.\n",
            "nalaina.\n",
            "penavier.\n",
            "amar.\n",
            "allettarye.\n",
            "zoyai.\n",
            "bvaunten.\n",
            "jahlenen.\n",
            "contari.\n",
            "benna.\n",
            "kamie.\n",
            "daycell.\n",
            "woxtelin.\n",
            "liy.\n",
            "maayonna.\n",
            "palaine.\n",
            "aoura.\n",
            "briarta.\n",
            "surahe.\n",
            "derlyn.\n",
            "kyriah.\n",
            "nicar.\n",
            "bersyd.\n",
            "ane.\n",
            "olani.\n",
            "jishah.\n",
            "sayda.\n"
          ]
        }
      ],
      "source": [
        "model.generate(42)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
